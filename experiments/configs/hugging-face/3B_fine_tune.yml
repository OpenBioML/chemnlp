---
# Dataset configuration (datasets.load_from_disk arguments)
data:
  path: /fsx/proj-chemnlp/data/EleutherAI/pythia-1b/marianna13/chemrxiv

# Model configuration (model.from_pretrained arguments)
model:
  base: GPTNeoXForCausalLM
  name: EleutherAI/pythia-2.8b
  revision: main # latest model

# Training strategies (PromptTuningConfig arguments)
prompt_tuning:
  enabled: false

# Training configuration (TrainerArguments from HF)
trainer:
  output_dir: /fsx/proj-chemnlp/experiments/checkpoints/finetuned/full_3b
  num_train_epochs: 1
  learning_rate: 3e-4
  evaluation_strategy: steps
  logging_steps: 50
  eval_steps: 10
  save_steps: 1000
  dataloader_num_workers: 4
  bf16: true
  fp16: false
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 1
  gradient_checkpointing: True
  deepspeed: /fsx/proj-chemnlp/jack/chemnlp/experiments/configs/hugging-face/deepspeed_offload_S3.json

# Logging configuration (WandB init arguments)
wandb:
  enabled: true
  project: LLCheM
  group: test
  name: test_3B_fine_tune
